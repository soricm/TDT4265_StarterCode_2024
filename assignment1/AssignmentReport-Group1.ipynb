{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report\n",
    "*Marijan Soric & Zan Stanonik*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "The gradient for Logistic Regression is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C^n}{\\partial w_i}&= \\frac{\\partial }{\\partial w_i} \\left(-\\sum_{k=1}^K y_k^n \\log(\\hat{y}^n_k) \\right)\\\\\n",
    "&=-\\sum_{k=1}^Ky_k^n \\frac{\\partial }{\\partial w_i} \\left( \\log(\\hat{y}^n_k) \\right)\\\\\n",
    "&=-\\sum_{k=1}^Ky_k^n \\frac{\\partial \\log(\\hat{y}^n_k)}{\\partial \\hat{y}^n_k} \\frac{\\partial \\hat{y}^n_k} {\\partial z_k}\\frac{\\partial z_k}{\\partial w_i} \\quad \\mathrm{Chain \\,rule}\\\\\n",
    "&=-\\sum_{k=1}^Ky_k^n \\frac{1}{\\hat{y}^n_k} \\cdot \\hat{y}^n_k (\\delta_{i,k}-\\hat{y}^n_i) \\cdot x^n_i \\quad \\mathrm{Result \\, from \\,} \\star\\\\\n",
    "&=-\\sum_{k=1}^K y_k^n (\\delta_{i,k}-\\hat{y}^n_i) \\cdot x^n_i\\\\\n",
    "&=- y_i^n (1-\\hat{y}^n_i) x^n_i+ \\sum_{k \\neq i}y_k^n \\hat{y}^n_i  x^n_i \\\\\n",
    "&=\\hat{y}^n_i   x^n_i \\underbrace{\\left( y_i^n+ \\sum_{k \\neq i} y_k^n \\right)}_{=1} - y_i^n x_i^n\\\\\n",
    "&=-(y_i^n-\\hat{y}^n_i) x_i^n\n",
    "\\end{aligned}\n",
    "\n",
    "Results $\\star$:\n",
    "\n",
    "Since $\\hat{y}^n_k=f(z_k)$, we have:\n",
    "$$\\frac{\\partial \\hat{y}^n_k}{\\partial z_i}=\\frac{\\partial }{\\partial z_i} \\left( \\frac{e^{z_k}}{\\sum_{k'} e^{z_{k'}}} \\right)=\\frac{\\delta_{i,k}e^{z_k}\\sum_{k'} e^{z_{k'}}- e^{z_k}e^{z_i}}{(\\sum_{k'} e^{z_{k'}})^2}\n",
    "= \\frac{e^{z_k}}{\\sum_{k'} e^{z_{k'}}} \\cdot \\frac{\\delta_{i,k} \\sum_{k'} e^{z_{k'}}- e^{z_i}}{\\sum_{k'} e^{z_{k'}}}= \\hat{y}^n_k (\\delta_{i,k}-\\hat{y}^n_i)$$\n",
    "\n",
    "And \n",
    "$$\\frac{\\partial z_k}{\\partial w_i} = \\frac{\\partial }{\\partial w_i} \\left( w_k^T \\cdot x^n \\right) = x_i^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "*After how many epochs does early stopping kick\n",
    "in?*   \n",
    "\n",
    "Early stopping kicks in at epoch 14 already, which just goes to show how fast the model can start to overfit. It is interesting to note that with random shuffling, it stops at 74, which just goes to show that this approaches are really valid to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2e)\n",
    "*Include a plot in your report of the validation accuracy with and without shuffle. You\n",
    "should notice that the validation accuracy has fewer ”spikes”. Why does this happen?*   \n",
    "This happens because the data is drawn randomly, and the model tries to learn more general features with less overfitting. Which eventually leads to less \"over shoots\" in the loss.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "*For your model trained in task 3c, do you notice any signs of overfitting? Explain\n",
    "your reasoning.*\n",
    "\n",
    "We can notice that the accuracy from the training set and validation set are similar (same order). And even the validation accuracy is higher than the training which is surprising. The overfitting case happens when $accuracy_{training}>>accuracy_{validation}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "The gradient for Logistic Regression is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w}&= \\frac{\\partial C}{\\partial w}+\\lambda \\frac{\\partial R}{\\partial w}\\\\\n",
    "\\frac{\\partial R}{\\partial w_{k,l}}&= \\frac{\\partial }{\\partial w_{k,l}} \\left(\\sum_{i,j} w_{i,j}^2 \\right)\\\\\n",
    "&= \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{k,l}} \\\\\n",
    "&= \\sum_{i,j} \\delta_{k,i} \\delta_{l,j}2 w_{i,j} \\\\\n",
    "&= 2 w_{k,l} \\\\\n",
    "\\end{aligned}\n",
    "Thus,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w_{k,l}}&= -\\frac{1}{N}\\sum_{n=1}^N \\left( x_l^n(y_k^n-\\hat{y}^n_k) \\right)  +2 \\lambda w_{k,l} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "*Visualize the weight for each digit for the two models. Why are the weights for the model with $\\lambda$ = 1.0 less noisy?*\n",
    "\n",
    "It is not clear on the picture... But, in theory, regularization helps to prevent overfitting. The weights can't have value that are too high because the regularization penalizes them (they are included in the gradient). The variance between weights is lower with regularization which means less noisy weights. \n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)\n",
    "\n",
    "![](compare_lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "*You will notice that the validation accuracy degrades when applying any amount\n",
    "of regularization. What do you think is the reason for this?*\n",
    "\n",
    "When $\\lambda$ is too high, the weights become very small and similar, then it degrades the model (low variance but high biais). This is the trade-off biais-variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "NB : There is a mistake here: we should have on x-axis $\\lambda$ and on y-axis $L_2$.\n",
    "\n",
    "As expected, $\\frac{\\partial \\lVert w \\rVert^2}{\\partial \\lambda}<0$. As calculated before, in the gradient descent, we have:\n",
    "$$\\frac{\\partial J}{\\partial w_{k,l}}= -\\frac{1}{N}\\sum_{n=1}^N \\left( x_l^n(y_k^n-\\hat{y}^n_k) \\right)  +2 \\lambda w_{k,l}$$\n",
    "So the gradient penalizes high value of weights, then weights are\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
