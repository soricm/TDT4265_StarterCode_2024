{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine tuning Fast R-CNN \n",
    "Following this tutorial:\n",
    "https://www.kaggle.com/code/yerramvarun/fine-tuning-faster-rcnn-using-pytorch"
   ],
   "id": "b645d9181c2ad85f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T18:04:57.869012Z",
     "start_time": "2024-04-27T18:04:52.491983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import frcnn.detect_utils as detect_utils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from frcnn.model import get_model\n",
    "\n",
    "model_name = \"v2\"\n",
    "threshold = 0.5\n",
    "image_name = \"./datasets/1_train-val_1min_aalesund_from_start/img1/000003.jpg\""
   ],
   "id": "690ad5c4ebf82d7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T18:05:00.313578Z",
     "start_time": "2024-04-27T18:04:57.871538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install pycocotools --quiet\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!git checkout v0.3.0\n",
    "\n",
    "# !cp vision/references/detection/utils.py ./\n",
    "# !cp vision/references/detection/transforms.py ./\n",
    "# !cp vision/references/detection/coco_eval.py ./\n",
    "# !cp vision/references/detection/engine.py ./\n",
    "# !cp vision/references/detection/coco_utils.py ./"
   ],
   "id": "92fd7e7151e6e63c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T18:05:01.130391Z",
     "start_time": "2024-04-27T18:05:00.315900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "# pip install python-engineio \n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ],
   "id": "511f643753f9d1cc",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'engine'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 28\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdetection\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfaster_rcnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FastRCNNPredictor\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# these are the helper libraries imported.\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# pip install python-engineio \u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_one_epoch, evaluate\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mT\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'engine'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets \n",
   "id": "be5d6afde9cb48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# defining the files directory and testing directory\n",
    "files_dir = '../input/fruit-images-for-object-detection/train_zip/train'\n",
    "test_dir = '../input/fruit-images-for-object-detection/test_zip/test'\n",
    "\n",
    "\n",
    "class FruitImagesDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, files_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.files_dir = files_dir\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
    "                     if image[-4:]=='.jpg']\n",
    "\n",
    "\n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = [_, 'apple','banana','orange']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.files_dir, img_name)\n",
    "\n",
    "        # reading the images and converting them to correct size and color    \n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "\n",
    "        # annotation file\n",
    "        annot_filename = img_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # cv2 image gives size as height x width\n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "\n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "\n",
    "        # bounding box\n",
    "        xmin = int(member.find('bndbox').find('xmin').text)\n",
    "        xmax = int(member.find('bndbox').find('xmax').text)\n",
    "\n",
    "        ymin = int(member.find('bndbox').find('ymin').text)\n",
    "        ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "        xmin_corr = (xmin/wt)*self.width\n",
    "        xmax_corr = (xmax/wt)*self.width\n",
    "        ymin_corr = (ymin/ht)*self.height\n",
    "        ymax_corr = (ymax/ht)*self.height\n",
    "        boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
    "\n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    \n",
    "        # getting the areas of the boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "    \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "    \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "    \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # image_id\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "\n",
    "            sample = self.transforms(image = img_res,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "    \n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "        return img_res, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "# check dataset\n",
    "dataset = FruitImagesDataset(files_dir, 224, 224)\n",
    "print('length of dataset = ', len(dataset), '\\n')\n",
    "\n",
    "# getting the image and target for a test index.  Feel free to change the index.\n",
    "img, target = dataset[78]\n",
    "print(img.shape, '\\n',target)\n"
   ],
   "id": "762e75f232238096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# function to visualize bounding boxes in the image\n",
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    for box in (target['boxes']):\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth = 2,\n",
    "                                 edgecolor = 'r',\n",
    "                                 facecolor = 'none')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# plotting the image with bboxes. Feel free to change the index\n",
    "img, target = dataset[25]\n",
    "plot_img_bbox(img, target)"
   ],
   "id": "d57d5f93c2c0b2e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ],
   "id": "68fe927aaad9d555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Send train=True fro training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(0.5),\n",
    "            # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "            ToTensorV2(p=1.0)\n",
    "        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            ToTensorV2(p=1.0)\n",
    "        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ],
   "id": "1c81e6c98055446b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=True))\n",
    "dataset_test = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# train test split\n",
    "test_split = 0.2\n",
    "tsize = int(len(dataset)*test_split)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ],
   "id": "3e1f1b4cd6092f8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to train on gpu if selected.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "# training for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n"
   ],
   "id": "fb2d5ce306a5cf19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63702edb7053b7ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "\n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "\n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')\n",
    "\n",
    "# pick one image from the test set\n",
    "img, target = dataset_test[5]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])[0]\n",
    "\n",
    "print('predicted #boxes: ', len(prediction['labels']))\n",
    "print('real #boxes: ', len(target['labels']))"
   ],
   "id": "cc90f468995b6537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('EXPECTED OUTPUT')\n",
    "plot_img_bbox(torch_to_pil(img), target)"
   ],
   "id": "2b59bb5f8977739",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a9caf4abc653d740",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
